{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b04a62-64d8-4fdf-bdaf-fb7648778123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prophetを活用したモデル構築\n",
    "\n",
    "Prophetを活用したモデルトレーニングおよびMLflowトラッキングを行います\n",
    "\n",
    "**要件**\n",
    "クラスタ Runtime 15.4 ML LTS以上を使用してください。\n",
    "\n",
    "<!-- %md\n",
    "### Model Building Using Prophet\n",
    "We will perform model training using Prophet and track the results with MLflow. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb9adf2-1199-4de4-a08b-fcc8e10a71e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src='https://github.com/komae5519pv/komae_dbdemos/blob/main/fine_grain_forecast_20241013/Customized_e2e_demand_forecasting/_image_for_notebook/model_train.png?raw=true' width='1200'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3550463a-843c-4686-902f-07f441c22f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa12d76-cc7c-4596-9763-888ce5623544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "需要予測で人気の高いライブラリ、[prophet](https://facebook.github.io/prophet/) を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d985732-74de-40ab-b87f-7bb981bba457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install prophet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3603a11-5170-46f5-8c33-3ebf1b026ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9028a172-01bb-4ed8-b895-e4f995d3b677",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_date, col\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.prophet\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from math import sqrt\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b10d4c9-afa0-4b21-ab55-37fc44df6a66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: 単一の予測をしてみる\n",
    "\n",
    "自動販売機とアイテムの組み合わせごとに予測を生成する前に、\n",
    "Prophetの使い方に慣れるために特定の自動販売機と商品の組み合わせを選択します。\n",
    "\n",
    "<!-- %md\n",
    "## Step 2: Build a Single Forecast\n",
    "\n",
    "Before generating forecasts for each store-item combination, select a specific store and item combination to get familiar with how Prophet works: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee25736-cd69-4cb8-a8ec-528e211ebd10",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve Data for a Single Item-vending_machine Combination"
    }
   },
   "outputs": [],
   "source": [
    "sql_statement = '''\n",
    "  SELECT\n",
    "    vm,\n",
    "    item,\n",
    "    CAST(ds as date) as ds,\n",
    "    y\n",
    "  FROM silver_train\n",
    "  WHERE vm=1 AND item=1    -- 単一店舗x単一商品\n",
    "  ORDER BY ds\n",
    "  '''\n",
    "\n",
    "# assemble dataset in Pandas dataframe\n",
    "history_pd = spark.sql(sql_statement).toPandas()\n",
    "\n",
    "# drop any missing records\n",
    "history_pd = history_pd.dropna()\n",
    "\n",
    "display(history_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af46d1a3-5920-44ea-bbd0-63290dae5dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "次に、prophetライブラリをインポートしますが、使用時にやや冗長になるため、環境のログ設定を調整する必要があります。\n",
    "\n",
    "<!-- %md\n",
    "Now, we will import the prophet library, but because it can be a bit verbose when in use, we will need to fine-tune the logging settings in our environment: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f57e37-f6bb-4cfd-8c0a-61378c5cc1d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Prophet Library"
    }
   },
   "outputs": [],
   "source": [
    "# disable informational messages from prophet\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "542c7d71-f44a-4086-89f2-113f37557526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "データを確認した結果、成長パターンは線形に設定し、週と年の季節性パターンを有効にします。売上が増えるにつれて季節性も強くなるようなので、季節性モードは乗法に設定します。\n",
    "\n",
    "<!-- %md\n",
    "Based on our review of the data, it looks like we should set our overall growth pattern to linear and enable the evaluation of weekly and yearly seasonal patterns. We might also wish to set our seasonality mode to multiplicative as the seasonal pattern seems to grow with overall growth in sales: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f1e2fb-9f77-4b22-87a9-da7e825b1eab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train Prophet Model"
    }
   },
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "model = Prophet(\n",
    "  interval_width=0.95,                # 信頼区間の幅を95%に設定（予測の不確実性範囲）\n",
    "  growth='linear',                    # トレンド成長モデルを線形（linear）に設定\n",
    "  daily_seasonality=False,            # 日次の季節性を無効化\n",
    "  weekly_seasonality=True,            # 週次の季節性を有効化\n",
    "  yearly_seasonality=True,            # 年次の季節性を有効化\n",
    "  seasonality_mode='multiplicative'   # 季節性の影響を「乗法的」に設定（トレンドが大きいほど季節性の変動が大きくなる）\n",
    "  )\n",
    "\n",
    "# fit the model to historical data\n",
    "model.fit(history_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b90af43-daa5-412b-8727-3c572e5ac66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "モデルのトレーニングが完了したので、それを使って90日間の予測を作成しましょう。\n",
    "\n",
    "<!-- %md\n",
    "Now that we have a trained model, let's use it to build a 90-day forecast: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a197c596-9b12-4422-8dbb-9374339727a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Forecast"
    }
   },
   "outputs": [],
   "source": [
    "# define a dataset including both historical dates & 90-days beyond the last available date\n",
    "# 将来予測用の日付データを生成\n",
    "future_pd = model.make_future_dataframe(\n",
    "  periods=90,             # 過去データの終了日から90日先までの日付を生成\n",
    "  freq='d',               # 日単位の頻度でデータを生成\n",
    "  include_history=True    # 過去データも含める（予測と過去データを比較可能にするため）\n",
    "  )\n",
    "\n",
    "# predict over the dataset\n",
    "forecast_pd = model.predict(future_pd)\n",
    "\n",
    "display(forecast_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f00204-7470-409d-b87f-51cb5a6288f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "モデルの結果はどうでしょうか？ここでは、モデルの全体的なトレンドと季節的なトレンドがグラフとして表示されています。\n",
    "\n",
    "<!-- %md\n",
    "How did our model perform? Here we can see the general and seasonal trends in our model presented as graphs: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4967e647-383d-4d90-90d0-e186baeeec18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Examine Forecast Components"
    }
   },
   "outputs": [],
   "source": [
    "trends_fig = model.plot_components(forecast_pd)\n",
    "display(trends_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1462ff-205d-4e03-9f49-5ab8312a94a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ここでは、実際のデータと予測データの比較、および将来の予測を見ることができます。  \n",
    "ただし、グラフが読みやすいように、過去1年分のデータに限定しています。\n",
    "\n",
    "<!-- %md\n",
    "And here, we can see how our actual and predicted data line up as well as a forecast for the future, though we will limit our graph to the last year of historical data just to keep it readable: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f72de7d-537b-4ccb-b06a-d9c0383517a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Historicals vs. Predictions"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the forecast results\n",
    "predict_fig = model.plot(forecast_pd, xlabel='date', ylabel='sales')\n",
    "\n",
    "# Adjust the graph: show the past year and 90-day forecast\n",
    "xlim = predict_fig.axes[0].get_xlim()                   # Get the current range of the X-axis\n",
    "new_xlim = (xlim[1] - (180.0 + 365.0), xlim[1] - 90.0)  # Set the new range for the X-axis\n",
    "predict_fig.axes[0].set_xlim(new_xlim)                  # Apply the new X-axis range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93097ea-5c49-45b9-a31b-81482cba4e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**補足:** 黒い点は実際のデータを表し、濃い青の線は予測、薄い青の帯は95%の予測区間を示しています。\n",
    "\n",
    "<!-- %md\n",
    "**NOTE** The black dots represent our actuals with the darker blue line representing our predictions and the lighter blue band representing our (95%) uncertainty interval. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ea78120-c987-4635-90c4-0c2f6d82597e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "視覚的な確認も有用ですが、予測モデルを適切に評価するために、  \n",
    "実績に対する予測値の平均絶対誤差（MAE）、平均二乗誤差（MSE）、および二乗平均平方根誤差（RMSE）の値を計算します。\n",
    "\n",
    "<!-- %md\n",
    "- Visual inspection is useful, but to properly evaluate the forecast model, we will calculate the Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) values for the predicted values compared to the actuals. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d434e5-1832-4ca2-a89e-af485d3a9f7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate Evaluation metrics"
    }
   },
   "outputs": [],
   "source": [
    "# get historical actuals & predictions for comparison\n",
    "actuals_pd = history_pd[ history_pd['ds'] < date(2018, 1, 1) ]['y']\n",
    "predicted_pd = forecast_pd[ forecast_pd['ds'] < pd.to_datetime('2018-01-01') ]['yhat']\n",
    "\n",
    "# calculate evaluation metrics\n",
    "mae = mean_absolute_error(actuals_pd, predicted_pd)   # 実績値と予測値の絶対誤差の平均 (数値が小さいほど良い予測 / 外れ値の影響を受けにくい)\n",
    "mse = mean_squared_error(actuals_pd, predicted_pd)    # 実績値と予測値の差を二乗して平均したもの (外れ値の影響を強く受ける)\n",
    "rmse = sqrt(mse)                                      # MSEの平方根を取ったもの(実績値と予測値の差の「標準的な大きさ」を示す)\n",
    "\n",
    "# print metrics to the screen\n",
    "print( '\\n'.join(['MAE: {0}', 'MSE: {1}', 'RMSE: {2}']).format(mae, mse, rmse) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a96cab4-94fa-41ed-8206-01e9fc1e22aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "prophetは、予測が時間経過とともにどのように維持されるかを評価するための[追加の手段](https://facebook.github.io/prophet/docs/diagnostics.html)を提供しています。  \n",
    "予測モデルを構築する際には、これらの手法の使用を強くお勧めしますが、今回はスケーリングの課題に焦点を当てるため、それらについては省略します。\n",
    "\n",
    "<!-- %md\n",
    "prophet provides [additional means](https://facebook.github.io/prophet/docs/diagnostics.html) for evaluating how your forecasts hold up over time. You're strongly encouraged to consider using these and those additional techniques when building your forecast models but we'll skip this here to focus on the scaling challenge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b0171d-1d92-4af4-8cbf-35c77d842a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: 予測生成のスケールアップ\n",
    "\n",
    "基本的な手順を理解したので、主目的である自動販売機とアイテムの組み合わせごとに多数の詳細なモデルと予測を構築してます。まず、自動販売機・アイテム・日付単位で販売データを集計します。\n",
    "\n",
    "**注意**: このデータセットのデータはすでにこの粒度で集計されているはずですが、期待通りのデータ構造を持っていることを確認するために、改めて明示的に集計します。\n",
    "\n",
    "\n",
    "<!-- %md\n",
    "## Step 3: Scale Forecast Generation\n",
    "\n",
    "With the mechanics under our belt, let's now tackle our original goal of building numerous, fine-grain models & forecasts for individual store and item combinations.  We will start by assembling sales data at the store-item-date level of granularity:\n",
    "\n",
    "**NOTE**: The data in this data set should already be aggregated at this level of granularity but we are explicitly aggregating to ensure we have the expected data structure. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a68c83-1673-46ac-879f-7c6347450acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3-1. トレーニングデータ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02810e8-e745-4fa0-bae0-8c9d78371639",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve Data for All Vending Machine-Item Combinations"
    }
   },
   "outputs": [],
   "source": [
    "sql_statement = '''\n",
    "  SELECT\n",
    "    vm,\n",
    "    item,\n",
    "    CAST(ds as date) as ds,\n",
    "    SUM(y) as y\n",
    "  FROM silver_train\n",
    "  GROUP BY vm, item, ds\n",
    "  ORDER BY vm, item, ds\n",
    "  '''\n",
    "\n",
    "vm_item_history = (\n",
    "  spark\n",
    "    .sql( sql_statement )\n",
    "    # .repartition(sc.defaultParallelism, ['vm', 'item'])\n",
    "    .repartition(sc.defaultParallelism, 'vm', 'item')\n",
    "  ).cache()\n",
    "\n",
    "# # 特徴量テーブルとして保存\n",
    "# vm_item_history.write.format('delta').mode('overwrite').saveAsTable('silver_train')\n",
    "\n",
    "display(vm_item_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998218cc-ae5d-4385-bee6-4414c03bb04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(vm_item_history.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b998f16-18b9-42dd-840a-b5ffd4201088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "自動販売機・アイテム・日付単位でデータを集計したので、これをどのようにprophetに渡すかを考えます。各自動販売機とアイテムの組み合わせごとにモデルを作るためには、データから自動販売機とアイテムのサブセットを渡し、そのサブセットでモデルを学習させ、予測を得ます。予測結果には、自動販売機とアイテムの識別子を保持し、Prophetモデルが生成した必要なフィールドだけが含まれます。\n",
    "\n",
    "<!-- %md\n",
    "With our data aggregated at the store-item-date level, we need to consider how we will pass our data to prophet. If our goal is to build a model for each store and item combination, we will need to pass in a store-item subset from the dataset we just assembled, train a model on that subset, and receive a store-item forecast back. We'd expect that forecast to be returned as a dataset with a structure like this where we retain the store and item identifiers for which the forecast was assembled and we limit the output to just the relevant subset of fields generated by the Prophet model: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbf685dc-339d-4da3-8e2a-f470e9488249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3-2. MLflow トラッキングのロジック定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c0ba340-126a-4c98-b7e4-f2ce6ec70660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "指定したカタログ、スキーマ配下のモデルを全て削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b25cbf-1ec1-4a22-b9e8-2e04736847af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delete Models from UC"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.sdk import WorkspaceClient\n",
    "# from mlflow.tracking import MlflowClient\n",
    "\n",
    "# def delete_specific_models(catalog_name, schema_name, prefix):\n",
    "#     workspace_client = WorkspaceClient()\n",
    "#     mlflow_client = MlflowClient()\n",
    "    \n",
    "#     models = workspace_client.registered_models.list(\n",
    "#         catalog_name=catalog_name,\n",
    "#         schema_name=schema_name\n",
    "#     )\n",
    "    \n",
    "#     for model in models:\n",
    "#         full_name = model.full_name\n",
    "#         if full_name.startswith(prefix):\n",
    "#             print(f\"Deleting model: {full_name}\")\n",
    "#             mlflow_client.delete_registered_model(name=full_name)\n",
    "#         else:\n",
    "#             pass\n",
    "#             # print(f\"Skipping model: {full_name}\")\n",
    "    \n",
    "#     print(\"Deletion process completed.\")\n",
    "\n",
    "# # 使用例\n",
    "# delete_specific_models(MY_CATALOG, MY_SCHEMA, f\"{MY_CATALOG}.{MY_SCHEMA}.demand_forecast_vm_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06c415e0-6b40-49ce-9b98-41c47b033edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Experimentを設定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed164a6-9167-4881-93aa-ff06255b10d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC配下のモデルとして登録\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ce18d28-b30f-4cf4-b097-4a5e1e2674b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MLflowのシステムメトリクスロギングを無効にします\n",
    "mlflow.disable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b22bca9-af17-4a0f-a268-a0ec36898d98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Experiment"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 現在の日付をyyyymmdd形式で取得\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Set MLflow Experiment\n",
    "experiment_name = f\"/Shared/{MY_CATALOG}_demand_forecast_{current_datetime}\"\n",
    "# experiment_name = f\"/Shared/komae_demand_forecast_{current_datetime}\"\n",
    "experiment_id = mlflow.set_experiment(experiment_name).experiment_id\n",
    "\n",
    "print(f'Experiment Name: {experiment_name}')\n",
    "print(f'Experiment ID: {experiment_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee2ace9-4cf3-414a-bbff-f1dd2bc2f172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "モデルをトレーニングして予測を作成するために、Pandas関数を使います。この関数は、自動販売機とアイテムごとに整理されたデータを受け取り、予測結果を返します。\n",
    "\n",
    "この関数では、モデルのトレーニングや予測の部分は以前と同じで、新しいのは予測結果をPandasでまとめる標準的な処理だけです。\n",
    "\n",
    "<!-- %md\n",
    "To train the model and generate a forecast we will leverage a Pandas function.  We will define this function to receive a subset of data organized around a store and item combination.  It will return a forecast in the format identified in the previous cell:\n",
    "\n",
    "**UPDATE** With Spark 3.0, pandas functions replace the functionality found in pandas UDFs.  The deprecated pandas UDF syntax is still supported but will be phased out over time.  For more information on the new, streamlined pandas functions API, please refer to [this document](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html).\n",
    "There's a lot taking place within our function, but if you compare the first two blocks of code within which the model is being trained and a forecast is being built to the cells in the previous portion of this notebook, you'll see the code is pretty much the same as before. It's only in the assembly of the required result set that truly new code is being introduced and it consists of fairly standard Pandas dataframe manipulations. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1860fad0-fa79-4280-b086-85c3b2417923",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train Model & Logging"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# median_absolute_percentage_error関数の定義\n",
    "def median_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# train_modelの戻り値のスキーマ定義\n",
    "result_schema = StructType([\n",
    "    StructField('ds', DateType()),\n",
    "    StructField('vm', IntegerType()),\n",
    "    StructField('item', IntegerType()),\n",
    "    StructField('y', FloatType()),\n",
    "    StructField('yhat', FloatType()),\n",
    "    StructField('yhat_upper', FloatType()),\n",
    "    StructField('yhat_lower', FloatType()),\n",
    "    StructField('mae', FloatType()),\n",
    "    StructField('mse', FloatType()),\n",
    "    StructField('rmse', FloatType()),\n",
    "    StructField('mape', FloatType()),\n",
    "    StructField('mdape', FloatType())\n",
    "])\n",
    "\n",
    "# Prophetモデルのトレーニングと予測、MLflowのロギング\n",
    "def train_model(history_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    グループごとにPrphetモデルトレーニング＆予測結果を戻す\n",
    "    '''\n",
    "    # ------------------------------\n",
    "    # GET METADATA\n",
    "    # ------------------------------\n",
    "    vm = history_pd['vm'].iloc[0]\n",
    "    item = history_pd['item'].iloc[0]\n",
    "    run_id = history_pd[\"run_id\"].iloc[0]  # Pulls run ID to do a nested run\n",
    "\n",
    "    # ------------------------------\n",
    "    # TRAIN THE MODEL\n",
    "    # ------------------------------\n",
    "    # ハイパーパラメータの設定\n",
    "    interval_width = 0.95\n",
    "    growth = 'linear'\n",
    "    daily_seasonality = False\n",
    "    weekly_seasonality = True\n",
    "    yearly_seasonality = True\n",
    "    seasonality_mode = 'multiplicative'\n",
    "\n",
    "    # Prophetモデルの設定と学習\n",
    "    model = Prophet(\n",
    "        interval_width=interval_width,\n",
    "        growth=growth,\n",
    "        daily_seasonality=daily_seasonality,\n",
    "        weekly_seasonality=weekly_seasonality,\n",
    "        yearly_seasonality=yearly_seasonality,\n",
    "        seasonality_mode=seasonality_mode\n",
    "    )\n",
    "    model.fit(history_pd)\n",
    "\n",
    "    # --------------------------\n",
    "    # BUILD FORECAST AS BEFORE\n",
    "    # --------------------------\n",
    "    # make predictions\n",
    "    future_pd = model.make_future_dataframe(\n",
    "        periods=90,\n",
    "        freq='d',\n",
    "        include_history=True\n",
    "    )\n",
    "    forecast_pd = model.predict(future_pd)\n",
    "\n",
    "    # --------------------------------------\n",
    "    # ASSEMBLE EXPECTED RESULT SET\n",
    "    # --------------------------------------\n",
    "    # get relevant fields from forecast\n",
    "    f_pd = forecast_pd[ ['ds', 'yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
    "\n",
    "    # get relevant fields from history\n",
    "    h_pd = history_pd[ ['ds', 'vm', 'item', 'y'] ].set_index('ds')\n",
    "\n",
    "    # join history and forecast\n",
    "    results_pd = f_pd.join( h_pd, how='left' )\n",
    "    results_pd.reset_index(level=0, inplace=True)\n",
    "  \n",
    "    # get vm & item from incoming data set\n",
    "    results_pd['vm'] = h_pd['vm'].iloc[0]\n",
    "    results_pd['item'] = h_pd['item'].iloc[0]\n",
    "\n",
    "    # メトリクス計算\n",
    "    actuals_pd = history_pd[history_pd['ds'] < pd.to_datetime('2018-01-01')]['y']\n",
    "    predicted_pd = forecast_pd[forecast_pd['ds'] < pd.to_datetime('2018-01-01')]['yhat']\n",
    "\n",
    "    if len(actuals_pd) > 0 and len(predicted_pd) > 0:\n",
    "        mae = mean_absolute_error(actuals_pd, predicted_pd)\n",
    "        mse = mean_squared_error(actuals_pd, predicted_pd)\n",
    "        rmse = sqrt(mse)\n",
    "        mape = mean_absolute_percentage_error(actuals_pd, predicted_pd)\n",
    "        mdape = median_absolute_percentage_error(actuals_pd, predicted_pd)\n",
    "\n",
    "        results_pd['mae'] = mae\n",
    "        results_pd['mse'] = mse\n",
    "        results_pd['rmse'] = rmse\n",
    "        results_pd['mape'] = mape\n",
    "        results_pd['mdape'] = mdape\n",
    "    else:\n",
    "        print(f\"Warning: No data available for metrics calculation for vm {vm} and item {item}\")\n",
    "        results_pd['mae'] = results_pd['mse'] = results_pd['rmse'] = results_pd['mape'] = results_pd['mdape'] = None\n",
    "\n",
    "    # ------------------------------\n",
    "    # RESUME THE TOP-LEVEL TRAINING\n",
    "    # ------------------------------\n",
    "    with mlflow.start_run(run_id=run_id) as outer_run:\n",
    "        # Small hack for running as a job\n",
    "        experiment_id = outer_run.info.experiment_id\n",
    "        print(f\"Current experiment_id = {experiment_id}\")\n",
    "\n",
    "        # Create a nested run for the vm and item\n",
    "        with mlflow.start_run(run_name=f\"vm_{vm}_item_{item}\", nested=True, experiment_id=experiment_id) as run:\n",
    "\n",
    "            # # ----------------------\n",
    "            # # LOG MODEL\n",
    "            # # ----------------------\n",
    "            # # シグネチャの推論\n",
    "            # signature = infer_signature(history_pd[['ds']], forecast_pd[['yhat', 'yhat_upper', 'yhat_lower']])\n",
    "            \n",
    "            # # モデルのロギング\n",
    "            # registered_model_name = f\"{MY_CATALOG}.{MY_SCHEMA}.demand_forecast_vm_{vm}_item_{item}\"\n",
    "            # mlflow.prophet.log_model(\n",
    "            #     model, \n",
    "            #     f\"prophet_model_vm_{vm}_item_{item}\",\n",
    "            #     signature=signature,\n",
    "            #     registered_model_name=registered_model_name\n",
    "            # )\n",
    "            # # エイリアスの設定\n",
    "            # client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "            # def get_latest_model_version(model_name):\n",
    "            #     model_version_infos = client.search_model_versions(f\"name = '{model_name}'\")\n",
    "            #     return max([model_version_info.version for model_version_info in model_version_infos])\n",
    "\n",
    "            # latest_version = get_latest_model_version(registered_model_name)\n",
    "            # client.set_registered_model_alias(registered_model_name, \"Champion\", latest_version)\n",
    "\n",
    "            # print(f\"Set 'Champion' alias to version {latest_version} of model {registered_model_name}\")\n",
    "\n",
    "            # ----------------------\n",
    "            # LOG TAG\n",
    "            # ----------------------\n",
    "            mlflow.set_tag(\"vm\", vm)\n",
    "            mlflow.set_tag(\"item\", item)\n",
    "            mlflow.set_tag(\"mlflow.note.content\", f\"Demand forecast model for VM {vm} and Item {item}\")\n",
    "\n",
    "            # ----------------------\n",
    "            # LOG HYPER PARAMETER\n",
    "            # ----------------------\n",
    "            mlflow.log_params({\n",
    "                \"interval_width\": model.interval_width,\n",
    "                \"growth\": model.growth,\n",
    "                \"daily_seasonality\": model.daily_seasonality,\n",
    "                \"weekly_seasonality\": model.weekly_seasonality,\n",
    "                \"yearly_seasonality\": model.yearly_seasonality,\n",
    "                \"seasonality_mode\": model.seasonality_mode\n",
    "            })\n",
    "\n",
    "            # ----------------------\n",
    "            # LOG METRICS\n",
    "            # ----------------------\n",
    "            # メトリクスのロギング\n",
    "            mlflow.log_metrics({\n",
    "                \"mae\": mae,\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"mape\": mape,\n",
    "                \"mdape\": mdape\n",
    "            })\n",
    "\n",
    "            # Create a return pandas DataFrame that matches the schema above\n",
    "            return_df = results_pd[['ds', 'vm', 'item', 'y', 'yhat', 'yhat_upper', 'yhat_lower', 'mae', 'mse', 'rmse', 'mape', 'mdape']]\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58850e03-93ee-4d93-97a2-ed74aa901a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3-3. Pandas関数を使って予測スタート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b7fe5a-d9f1-4188-89c4-9ff026f2534d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "では、Pandas関数を使って予測を作成しましょう。  \n",
    "自動販売機とアイテムごとにデータをグループ化し、関数を適用します。そして、データ管理用に今日の日付を *training_date* として追加します。Pandas UDFではなく `applyInPandas()` を使います。\n",
    "\n",
    "\n",
    "<!-- %md\n",
    "Now let's call our pandas function to build our forecasts.  We do this by grouping our historical dataset around store and item.  We then apply our function to each group and tack on today's date as our *training_date* for data management purposes:\n",
    "\n",
    "**UPDATE** Per the previous update note, we are now using applyInPandas() to call a pandas function instead of a pandas UDF. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c38d57a-3cf4-4d09-8fa2-5366d2f9d018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"demand_forecast\") as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    model_directories_df = (\n",
    "        vm_item_history\n",
    "            .withColumn(\"run_id\", F.lit(run_id)) # add current run_id\n",
    "            .groupBy('vm', 'item')\n",
    "            .applyInPandas(train_model, schema=result_schema)\n",
    "            .withColumn('training_date', F.current_date())\n",
    "            .cache()\n",
    "    )\n",
    "\n",
    "vm_item_history = vm_item_history.drop(col('y'))\n",
    "combined_df = model_directories_df.join(vm_item_history, on=[\"ds\", \"vm\", \"item\"], how=\"left\")\n",
    "\n",
    "combined_df.createOrReplaceTempView('new_forecasts')\n",
    "\n",
    "# display(combined_df.count())\n",
    "# display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340ea27e-c3b6-439a-b6d4-5690d00d838e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3-4. 予測結果をテーブル保存\n",
    "\n",
    "多くの場合、予測結果をレポートで活用したいと考えるので、クエリー可能なテーブル構造として保存します。\n",
    "\n",
    "以下では、他のユーザーとデータベースが競合しないようにユーザー名を埋め込んだデータベースを作成しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92008c66-e8c3-4f03-bd78-6a6a10797bc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ハンズオン用にユーザー単位の設定"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Username を取得\n",
    "# username_raw = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "\n",
    "# # Username の英数字以外を除去し、全て小文字化\n",
    "# username = re.sub('[^A-Za-z0-9]+', '', username_raw).lower()\n",
    "\n",
    "# # ユーザー固有のデータベース名を生成します\n",
    "# db_name = f\"forecasts_{username}\"\n",
    "\n",
    "# # データベースの準備\n",
    "# spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
    "# spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "# spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "# # データベースを表示\n",
    "# print(f\"database_name: {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62200412-779a-446e-bf44-8f95b1ff08ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Persist Forecast Output"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- 予測結果テーブルの作成\n",
    "CREATE TABLE IF NOT EXISTS silver_forecasts (\n",
    "  order_date DATE,\n",
    "  vending_machine_id INTEGER,\n",
    "  item_id INTEGER,\n",
    "  actual_sales_quantity FLOAT,\n",
    "  forecast_sales_quantity FLOAT,\n",
    "  forecast_sales_quantity_upper FLOAT,\n",
    "  forecast_sales_quantity_lower FLOAT,\n",
    "  sales_inference_date DATE\n",
    "  )\n",
    "USING DELTA\n",
    "PARTITIONED BY (order_date);\n",
    "\n",
    "-- 予測結果テーブルにデータをマージ\n",
    "MERGE INTO silver_forecasts f\n",
    "USING new_forecasts n \n",
    "ON f.order_date = n.ds AND f.vending_machine_id = n.vm AND f.item_id = n.item\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  f.order_date = n.ds,\n",
    "  f.vending_machine_id = n.vm,\n",
    "  f.item_id = n.item,\n",
    "  f.actual_sales_quantity = n.y,\n",
    "  f.forecast_sales_quantity = n.yhat,\n",
    "  f.forecast_sales_quantity_upper = n.yhat_upper,\n",
    "  f.forecast_sales_quantity_lower = n.yhat_lower,\n",
    "  f.sales_inference_date = current_date()\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "  order_date,\n",
    "  vending_machine_id,\n",
    "  item_id,\n",
    "  actual_sales_quantity,\n",
    "  forecast_sales_quantity,\n",
    "  forecast_sales_quantity_upper,\n",
    "  forecast_sales_quantity_lower,\n",
    "  sales_inference_date)\n",
    "VALUES (\n",
    "  n.ds,\n",
    "  n.vm,\n",
    "  n.item,\n",
    "  n.y,\n",
    "  n.yhat,\n",
    "  n.yhat_upper,\n",
    "  n.yhat_lower,\n",
    "  current_date());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1681ac-8c9a-4b09-862b-49674108ee9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "コメント追加"
    }
   },
   "outputs": [],
   "source": [
    "# テーブル名\n",
    "table_name = f'{MY_CATALOG}.{MY_SCHEMA}.silver_forecasts'\n",
    "\n",
    "# テーブルコメント\n",
    "comment = \"\"\"\n",
    "`silver_forecasts`テーブルは、自動販売機の需要予測結果データを管理します。\n",
    "\"\"\"\n",
    "spark.sql(f'COMMENT ON TABLE {table_name} IS \"{comment}\"')\n",
    "\n",
    "# カラムコメント\n",
    "column_comments = {\n",
    "    \"order_date\": \"受注日（主キー、外部キー）、YYYY-MM-DDフォーマット\",\n",
    "    \"vending_machine_id\": \"自動販売機ID（主キー、外部キー）、例: 10\",\n",
    "    \"item_id\": \"商品ID（主キー、外部キー）、例: 10\",\n",
    "    \"actual_sales_quantity\": \"実績販売数、例: 50\",\n",
    "    \"forecast_sales_quantity\": \"予測販売数、例: 50\",\n",
    "    \"forecast_sales_quantity_upper\": \"予測販売数（上限）、例: 60\",\n",
    "    \"forecast_sales_quantity_lower\": \"予測販売数（下限）、例: 40\",\n",
    "    \"sales_inference_date\": \"販売数予測日、YYYY-MM-DDフォーマット\"\n",
    "}\n",
    "\n",
    "for column, comment in column_comments.items():\n",
    "    # シングルクォートをエスケープ\n",
    "    escaped_comment = comment.replace(\"'\", \"\\\\'\")\n",
    "    sql_query = f\"ALTER TABLE {table_name} ALTER COLUMN {column} COMMENT '{escaped_comment}'\"\n",
    "    spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af3c64b-bfd2-468a-a517-679ed7c9de2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3-5. 予測結果の可視化\n",
    "\n",
    "ここまでで、それぞれの店舗・商品の組み合わせに対する予測結果を生成し、それぞれの基本的な評価メトリクスを計算しました。  \n",
    "この予測データを確認するために、シンプルなクエリー(ここでは店舗1から店舗3における商品1に限定しています)を実行できます\n",
    "\n",
    "<!-- %md\n",
    "\n",
    "### 3-5. Visualization of Forecast Results\n",
    "\n",
    "At this point, we have generated forecast results for each store-item combination and calculated the basic evaluation metrics for each.  \n",
    "To review this forecast data, you can run a simple query (here limited to product 1 across stores 1 to 3). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ec42fa-97ec-4887-b30b-13829a7b732f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT\n  vending_machine_id,\n  order_date,\n  forecast_sales_quantity,\n  forecast_sales_quantity_upper,\n  forecast_sales_quantity_lower\nFROM silver_forecasts a\nWHERE item_id = 1 AND\n      vending_machine_id IN (1, 2, 3) AND\n      order_date >= '2018-01-01' AND\n      sales_inference_date=current_date()\nORDER BY vending_machine_id) SELECT `order_date`,SUM(`forecast_sales_quantity`) `column_5e4eb4575`,`vending_machine_id` FROM q GROUP BY `order_date`,`vending_machine_id`",
       "commandTitle": "可視化 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "vending_machine_id",
             "id": "column_5e4eb4573"
            },
            "x": {
             "column": "order_date",
             "id": "column_5e4eb4571"
            },
            "y": [
             {
              "column": "forecast_sales_quantity",
              "id": "column_5e4eb4575",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_5e4eb4575": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "7b716484-1636-4f03-b073-f41227aa3f21",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 39.72265625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "order_date",
           "type": "column"
          },
          {
           "column": "vending_machine_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "order_date",
           "type": "column"
          },
          {
           "alias": "column_5e4eb4575",
           "args": [
            {
             "column": "forecast_sales_quantity",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "vending_machine_id",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT\n",
    "  vending_machine_id,\n",
    "  order_date,\n",
    "  forecast_sales_quantity,\n",
    "  forecast_sales_quantity_upper,\n",
    "  forecast_sales_quantity_lower\n",
    "FROM silver_forecasts a\n",
    "WHERE item_id = 1 AND\n",
    "      vending_machine_id IN (1, 2, 3) AND\n",
    "      order_date >= '2018-01-01' AND\n",
    "      sales_inference_date=current_date()\n",
    "ORDER BY vending_machine_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fdbf6de-e809-421d-9a02-11ab875d3dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3-6. モデル予測結果の評価\n",
    "\n",
    "<!-- %md\n",
    "\n",
    "### 3-6. Evaluation of Model Forecast Results -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d181cd3e-deec-4a11-96f7-e859d026526e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "しかし、各予測の精度はどれくらい良い（または悪い）のでしょうか？Pandas関数を使うことで、各店舗・アイテムの予測に対する評価指標を以下のように生成できます。\n",
    "\n",
    "<!-- %md\n",
    "But how good (or bad) is each forecast?  Using the pandas function technique, we can generate evaluation metrics for each store-item forecast as follows -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3638c209-b408-4b7d-9825-6297ba8cef47",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Apply Same Techniques to Evaluate Each Forecast"
    }
   },
   "outputs": [],
   "source": [
    "# 予測結果のスキーマを定義\n",
    "eval_schema = StructType([\n",
    "  StructField('training_date', DateType()),  # トレーニング日時\n",
    "  StructField('vm', IntegerType()),          # 店舗\n",
    "  StructField('item', IntegerType()),        # アイテム\n",
    "  StructField('mae', FloatType()),           # 平均絶対誤差 (MAE)\n",
    "  StructField('mse', FloatType()),           # 平均二乗誤差 (MSE)\n",
    "  StructField('rmse', FloatType()),          # 二乗平均平方根誤差 (RMSE)\n",
    "  StructField('mape', FloatType()),          # 平均絶対パーセント誤差 (MAPE)\n",
    "  StructField('mdape', FloatType())          # 中央絶対パーセント誤差 (MdAPE)\n",
    "])\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mdape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# メトリクスを計算する関数を定義\n",
    "def evaluate_forecast(evaluation_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "  \n",
    "  # 入力データセットから店舗と商品を取得\n",
    "  training_date = evaluation_pd['training_date'].iloc[0]\n",
    "  vm = evaluation_pd['vm'].iloc[0]\n",
    "  item = evaluation_pd['item'].iloc[0]\n",
    "\n",
    "  # 評価メトリクスを計算\n",
    "  mae = mean_absolute_error(evaluation_pd['y'], evaluation_pd['yhat'])\n",
    "  mse = mean_squared_error(evaluation_pd['y'], evaluation_pd['yhat'])\n",
    "  rmse = sqrt(mse)\n",
    "  mape_value = mape(evaluation_pd['y'], evaluation_pd['yhat'])\n",
    "  mdape_value = mdape(evaluation_pd['y'], evaluation_pd['yhat'])\n",
    "\n",
    "  # 結果セットを組み立てる\n",
    "  results = {\n",
    "    'training_date': [training_date], \n",
    "    'vm': [vm], \n",
    "    'item': [item], \n",
    "    'mae': [mae], \n",
    "    'mse': [mse], \n",
    "    'rmse': [rmse],\n",
    "    'mape': [mape_value],\n",
    "    'mdape': [mdape_value]\n",
    "  }\n",
    "  return pd.DataFrame.from_dict(results)\n",
    "\n",
    "# メトリクスを計算\n",
    "results = (\n",
    "  spark\n",
    "    .table('new_forecasts')\n",
    "    .filter('ds < \\'2018-01-01\\'') # 過去のデータがある期間に絞って評価を実施\n",
    "    .select('training_date', 'vm', 'item', 'y', 'yhat')\n",
    "    .groupBy('training_date', 'vm', 'item')\n",
    "    .applyInPandas(evaluate_forecast, schema=eval_schema)\n",
    "    )\n",
    "\n",
    "results.createOrReplaceTempView('new_forecast_evals')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82410bb0-14a4-427e-8e03-234a8cce2ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "再び、各予測の評価指標をクエリ可能なテーブルとして保存します。\n",
    "\n",
    "<!-- %md\n",
    "Once again, we will likely want to report the metrics for each forecast, so we persist these to a queryable table: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d76a1f7-70f8-42a5-8fe9-07c52d3f2094",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Persist Evaluation Metrics"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS silver_forecast_evals (\n",
    "  vending_machine_id INTEGER,\n",
    "  item_id INTEGER,\n",
    "  mae FLOAT,\n",
    "  mse FLOAT,\n",
    "  rmse FLOAT,\n",
    "  mape FLOAT,\n",
    "  mdape FLOAT,\n",
    "  training_date date\n",
    "  )\n",
    "USING DELTA\n",
    "PARTITIONED BY (training_date);\n",
    "\n",
    "INSERT INTO silver_forecast_evals\n",
    "SELECT\n",
    "  vm as vending_machine_id,\n",
    "  item as item_id,\n",
    "  mae,\n",
    "  mse,\n",
    "  rmse,\n",
    "  mape,\n",
    "  mdape,\n",
    "  training_date\n",
    "FROM new_forecast_evals;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7ced60-ec69-437e-9153-72f0496583ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "コメント追加"
    }
   },
   "outputs": [],
   "source": [
    "# テーブル名\n",
    "table_name = f'{MY_CATALOG}.{MY_SCHEMA}.silver_forecast_evals'\n",
    "\n",
    "# テーブルコメント\n",
    "comment = \"\"\"\n",
    "`silver_forecast_evals`テーブルは、自動販売機の需要予測結果データを管理します。\n",
    "\"\"\"\n",
    "spark.sql(f'COMMENT ON TABLE {table_name} IS \"{comment}\"')\n",
    "\n",
    "# カラムコメント\n",
    "column_comments = {\n",
    "    \"vending_machine_id\": \"自動販売機ID（主キー、外部キー）、例: 10\",\n",
    "    \"item_id\": \"商品ID（主キー、外部キー）、例: 10\",\n",
    "    \"mae\": \"MAE、例: 6539.265137\",\n",
    "    \"mse\": \"MSE、例: 67675384.33\",\n",
    "    \"rmse\": \"RMSE、例: 8226.504883\",\n",
    "    \"mape\": \"MAPE、例: 10.95132351\",\n",
    "    \"mdape\": \"MDAPE、例: 8.96886158\",\n",
    "    \"training_date\": \"トレーニング実行日、YYYY-MM-DDフォーマット\"\n",
    "}\n",
    "\n",
    "for column, comment in column_comments.items():\n",
    "    # シングルクォートをエスケープ\n",
    "    escaped_comment = comment.replace(\"'\", \"\\\\'\")\n",
    "    sql_query = f\"ALTER TABLE {table_name} ALTER COLUMN {column} COMMENT '{escaped_comment}'\"\n",
    "    spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a3012e-245f-4aee-8474-847bf767fd8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "これらの予測について、それぞれの信頼性を評価するための指標を取得できます。\n",
    "\n",
    "<!-- %md\n",
    "And for each of these, we can retrieve a measure of help us assess the reliability of each forecast: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb0f38f-132a-4c42-af2a-9f58ca8eb624",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve Evaluation Metrics"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT\n",
    "  vending_machine_id,\n",
    "  mae,\n",
    "  mse,\n",
    "  rmse,\n",
    "  mape,\n",
    "  mdape\n",
    "FROM silver_forecast_evals a\n",
    "WHERE item_id = 1 AND\n",
    "      training_date=current_date()\n",
    "ORDER BY vending_machine_id"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1436459421668762,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "05_model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
